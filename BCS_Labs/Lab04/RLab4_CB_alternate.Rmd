---
title: "R Coding Lab Part 4"
output: rmdformats::downcute
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Complete the following lab as a group. This document should exist in your GitHub repo while you're working on it. Your code should be heavily commented so someone reading your code can follow along easily. See the first code snippet below for an example of commented code.**

**Here's the catch: For any given problem, the person writing the code should not be the person commenting that code, and every person must both code AND comment at least one problem in this lab (you decide how to split the work). This will involve lots of pushing and pulling through Git, and you may have to resolve conflicts if you're not careful! Refer to last Thursday's class notes for details on conflict resolution.**

**ALSO, all plots generated should have labeled axes, titles, and legends when appropriate. Don't forget units of measurement! Make sure these plots could be interpreted by your client.**

These problems were adapted from **Cleaning Data for Effective Data Science** by David Mertz

# Dealing With Outliers

The Michelson–Morley experiment was an attempt in the late 19th century to detect the existence of the luminiferous aether, a widely assumed medium that would carry light waves. This was the most famous “failed experiment” in the history of physics in that it did not detect what it was looking for—something we now know not to exist at all.

The general idea was to measure the speed of light under different orientations of the equipment relative to the direction of movement of the Earth, since relative movement of the ether medium would add or subtract from the speed of the wave. Yes, it does not work that way under the theory of relativity, but it was a reasonable guess 150 years ago.

Apart from the physics questions, the dataset derived by the Michelson–Morley experiment is widely available, including the sample given in `morley.dat`. The specific numbers in this data are measurements of the speed of light in km/s with a zero point of 299,000. So, for example, the mean measurement in experiment 1 was 299,909 km/s (you can check this when you load the data).

1) Using R to identify the outliers first within each setup (defined by the `Expt` number) and then within the data collection as a whole. The hope in the original experiment was that each setup would show a significant difference in central tendency. We did not cover confidence levels and null hypotheses, so simply create visualization(s) that aids you in gaining insight into how much apparent difference exists between the several setups.

```{r}
library(dplyr)
library(ggplot2)
```

```{r}
data <- read.csv("humans-names.csv")
morleydata <- read.table("morley.dat",header=T)
morley = read.table("morley.dat",header=T)

```

```{r}
expt = unique(morley$Expt)
expt.i = expt[1]
mu = c()
std = c()
idx.list = list()
for(expt.i in expt)
{
  morley.i = morley[morley$Expt==expt.i,]
  print(ggplot(morley.i,aes(y=Speed)) + geom_boxplot() + ggtitle(expt.i))
  q1 = quantile(morley.i$Speed, 0.25)
  q3 = quantile(morley.i$Speed, 0.75)
  IQR = 1.5*(q3-q1)
  
  idx = which(morley.i$Speed < (q1 - IQR) | morley.i$Speed > (q3+IQR))
  #print(IQR)
  idx.list = c(idx.list, list(idx)) 
  print(morley.i$Speed[idx])
  
}
ggplot(morley,aes(y=Speed)) + geom_boxplot() + ggtitle("Entire dataset")
```
```{r}
morleydata$Expt <- as.factor(morleydata$Expt)

head(morleydata)
#morley2$Expt = as.factor(morley2$Expt)
morleydata%>% ggplot(aes(x = Speed, fill = Expt))+
  geom_density(alpha = 0.3)
```


```{r}

i=1
index = c()
for (expt.i in expt)
{
  idx.expt = which(morley$Expt==expt.i)
  idx = idx.expt[1] + idx.list[[i]]-1
  i=i+1
  index = c(index,idx)
}
index = setdiff(1:nrow(morley),index)
morley2 = morley[index,]
```

2) If you discard the outliers within each setup, are the differences between setups increased or decreased? Answer with either a visualization or by looking at statistics on the reduced groups.
```{r}
for(expt.i in expt)
{
  morley.i = morley2[morley2$Expt==expt.i,]
  print(ggplot(morley.i,aes(y=Speed)) + geom_boxplot() + ggtitle(expt.i))
}
```


```{r}
morley2$Expt = as.factor(morley2$Expt)
morley2%>% ggplot(aes(x = Speed, fill = Expt))+
  geom_density(alpha = 0.3)

```

The differences are reduced but substantial differences remain.

# Mispelled Names
Our data set `humans-names.csv` contains 25,000 height and weight measurements. Each row has a person’s first name pulled from the US Social Security Agency list of common first names over the last century.

Unfortunately, our hypothetical data collectors for this dataset are simply terrible typists, and they make typos when entering names with alarming frequency. There are some number of intended names in this dataset, but quite a few simple miscodings of those names as well. Your goal is to clean up these mispelled names.

1) Identify every genuine name and correct all the misspelled ones to the correct canonical spelling. Use all the data wrangling tools you'd like (e.g. `dplyr` functions), but make sure you're checking each reassignment to make sure the names get classified correctly. You'll fully automate this process later. It is probably reasonable to assume that rare spellings are typos, at least if they are also relatively similar to common spellings.  
Hint: There are a number of ways to measure the similarity of strings and that provide a clue as to likely typos. One general class of approach is in terms of edit distance between strings, which describes how many edititing operations need to be done to tranform one string into another. The R package `stringdist` provides Damerau–Levenshtein, Hamming, Levenshtein, and optimal string alignment as measures of edit distance. Keep in mind that sometimes multiple legitimate names are actually close to each other in terms of similarity measures (Dan VS Don, Jacob VS Jakob, etc). If you want to use `stringdist` for this problem, start by looking at the functions `stringdist()` and `stringdistmatrix()`.
```{r}

library(stringdist)
data = read.csv('humans-names.csv')
names = unique(data$Name)
data.2 = data
correct.names = c('James','David','Barbara','John','Michael','William','Elizabeth','Joseph','Jessica','Susan','Patricia','Robert','Linda','Mary','Marie','Jennifer','Richard','Jon')
names.mat = outer(correct.names,names,function(x,y){stringdist(x,y)})
idx = apply(names.mat,2,function(x){which(x<=2)})
for(i in 1:length(names))
{
  if(length(idx[[i]])==1)
    data.2$Name[data$Name==names[i]] = correct.names[idx[[i]]]
  else
  {  
    if(names[i]=='eJohn')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='ohn')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='Jeon')
      data.2$Name[data$Name==names[i]] = 'Jon'
    else if(names[i]=='Johen')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='Mari')
      data.2$Name[data$Name==names[i]] = 'Marie'
    else if(names[i]=='oJhn')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='Joh')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='Maire')
      data.2$Name[data$Name==names[i]] = 'Marie'
    else if(names[i]=='on')
      data.2$Name[data$Name==names[i]] = 'Jon'
    else if(names[i]=='Marei')
      data.2$Name[data$Name==names[i]] = 'Marie'
    else if(names[i]=='Mar')
      data.2$Name[data$Name==names[i]] = 'Mary'
     else if(names[i]=='Jhn')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='Maire')
      data.2$Name[data$Name==names[i]] = 'Marie'
    else if(names[i]=='Jn')
      data.2$Name[data$Name==names[i]] = 'Jon'
    else if(names[i]=='Marey')
      data.2$Name[data$Name==names[i]] = 'Mary'
    else if(names[i]=='Jhon')
      data.2$Name[data$Name==names[i]] = 'John'
     else if(names[i]=='Jonh')
      data.2$Name[data$Name==names[i]] = 'John'
    else if(names[i]=='Joen')
      data.2$Name[data$Name==names[i]] = 'Jon'
    else if(names[i]=='Jo')
      data.2$Name[data$Name==names[i]] = 'Jon'
   
  }
}
print(data.2$Name[!data.2$Name%in%correct.names])
```


2) For each of the genuine names identified in (1), produce a histogram showing the distribution of Damerau–Levenshtein distances from the genuine name to the miscassified data. Make sure distances from genuine names to other genuine names are not included in these distributions.  
Arrange all of the histograms into one figure write a short interpretation of it intended for a non-statistician client. 
```{r}
names = data$Name[!data$Name%in%correct.names]
correct = data.2$Name[!data$Name%in%correct.names]
dist = data.frame(correct,stringdist(names,correct,method='dl'))
colnames(dist)= c('Name','distance')
ggplot(dist,aes(x=distance,group=Name)) + geom_histogram(binwidth=1)+xlab('Number of mistakes')+facet_wrap(Name~.) +ggtitle('Histogram of number of spelling mistakes for each name.')
```
Most names can be fixed with a single edit, but several names need 2 edits to fix.  None of the names need more than 2 edits.


3) Write code that reclassifies names similar to problem (1), but fully automated. You should end up with a function that takes the original data set and returns a cleaned version. Compare this cleaned data frame to the one from problem (1) and quantify the accuracy (i.e. what proportion of rows match?). Make sure your automated process achieves 90%, but shoot for higher if possible! 
```{r}
names = unique(data$Name)
clean.data = function(data)
{
  data.3 = data
  #correct.names = c('James','David','Barbara','John','Michael','William','Elizabeth','Joseph','Jessica','Susan','Patricia','Robert','Linda','Mary','Marie','Jennifer','Richard','Jon')
  correct.names = unique(data$Name[1:100])
  names.mat = outer(correct.names,names,function(x,y){stringdist(x,y)})
  idx = apply(names.mat,2,function(x){which(x<=2)})
  for(i in 1:length(names))
  {
    data.3$Name[data$Name==names[i]] = correct.names[idx[[i]]][1]
  }
  return(data.3) 
}
data.3 = clean.data(data)
acc=length(data.3$Name[data.3$Name==data.2$Name])/length(data.3$Name)*100
print('Accuracy:')
print(sprintf('%f%s',acc,'%'))
```

